#!/usr/bin/env python3
"""
Simulation-Based Calibration of rho Estimation Method
======================================================

PURPOSE: Can our SCOSYA estimation method recover known rho values
from synthetic data generated by the ABM?

PROTOCOL (written before any results were seen):
-------------------------------------------------

Design:
  True rho values: {0.2, 0.3, 0.5, 0.7, 1.0}
  Features per replicate: 258 (matching SCOSYA)
  Initial user fractions: Uniform(0.01, 0.99) for each feature
  Niche width: 0.10 for all features (moderate; matches sweep experiment)
  Population: 200 agents per feature (smaller for speed; 258*5*10 ABM runs)
  Steps: 200 per feature
  Interactions per step: 200 (= n_agents)
  Network: random mixing (no community structure)

Age stratification (apparent-time analogy):
  Run each feature's ABM for 200 steps total.
  "Old generation" = acceptance rate at step 100.
  "Young generation" = acceptance rate at step 200.
  Acceptance rate = fraction of agents with confidence > 0.5.

  Rationale: SCOSYA old speakers formed grammar ~40yrs ago, young ~15yrs
  ago. The half/full run approximates this: the state at step 100 is
  the grammar the "old generation" acquired; at step 200, the "young."

Estimation method (IDENTICAL to SCOSYA analysis):
  1. For each feature, compute delta = young_rate - old_rate.
  2. "Declining" = delta < -0.01 (young accept less than old; small
     threshold to avoid noise).
  3. "Near tipping" = young_rate in [0.20, 0.50].
  4. rho_hat = median of {f/(1-f)} for qualifying features.
  5. If no features qualify, replicate returns NaN.

Monte Carlo replicates: 20 per true rho value.
Random seeds: 1000 * rho_index + replicate_number (deterministic).

Evaluation metrics:
  - Median estimated rho across replicates, with IQR
  - Bias = median(rho_hat) - rho_true
  - Number of qualifying features per replicate (if too few, method fragile)
  - Recovery diagnostic: can we distinguish adjacent rho values?

PRE-REGISTERED PREDICTIONS (written before running):
----------------------------------------------------

1. POSITIVE BIAS AT LOW rho. At rho=0.2, the true critical mass is ~17%.
   Our selection window [0.20, 0.50] misses the actual tipping point
   (below 20%). The features we select will be those slightly ABOVE the
   tipping point that are declining slowly. Their f/(1-f) will overestimate
   rho.

2. BEST RECOVERY AT rho ~ 0.5. Here the critical mass is ~33%, nicely
   centered in our [0.20, 0.50] window. This is where we expect the
   least bias.

3. UNDERESTIMATION AT rho = 1.0. The critical mass is 50%, at the very
   edge of our window. Features near the tipping point will be at the
   upper boundary, and many genuinely tipping features will be excluded
   (young_rate > 0.50). The method should underestimate.

4. WIDE IQRs EVERYWHERE. The method relies on a subset of features that
   happen to be near the tipping point. With 258 features uniformly
   distributed, only ~20% will land near any given tipping point. With
   additional filtering for "declining," the qualifying set may be <30
   features. Sampling variability will be high.

5. FEW QUALIFYING FEATURES AT EXTREME rho. At rho=0.2 and rho=1.0,
   fewer features will pass both the "declining" and "near tipping"
   filters. Some replicates may have zero qualifying features.

WHAT WOULD CHANGE OUR CONCLUSIONS:
-----------------------------------

If the method CANNOT distinguish rho=0.3 from rho=0.7 (i.e., their
confidence intervals overlap), then our SCOSYA estimate of rho ~ 0.5
is uninformative: it doesn't rule out most of the parameter space.

If the method shows STRONG BIAS in one direction, our SCOSYA estimate
should be corrected by that bias (e.g., if estimation systematically
overestimates by 0.1, the corrected estimate is rho ~ 0.4).

If the method has NO qualifying features for >50% of replicates at
any rho, the SCOSYA estimation approach is fundamentally fragile and
we cannot trust the empirical estimate.

Usage:
    python models/fake_data_calibration.py
    python models/fake_data_calibration.py --mc-runs 10  # faster
    python models/fake_data_calibration.py --no-viz       # text only
"""

import random
import math
import time
import statistics
import argparse
from typing import List, Tuple, Optional, Dict

# Import ABM components from the main model
import sys
import os
sys.path.insert(0, os.path.dirname(os.path.abspath(__file__)))
from grammaticality_abm import Construction, World

try:
    import matplotlib
    matplotlib.use('Agg')
    import matplotlib.pyplot as plt
    import numpy as np
    HAS_MPL = True
except ImportError:
    HAS_MPL = False


# ============================================================================
# CONFIGURATION (fixed before running)
# ============================================================================

TRUE_RHO_VALUES = [0.2, 0.3, 0.5, 0.7, 1.0]
N_FEATURES = 258          # match SCOSYA
N_AGENTS = 200            # smaller for speed
N_STEPS = 200             # total steps
OLD_SNAPSHOT = 100        # "old generation" snapshot
YOUNG_SNAPSHOT = 200      # "young generation" snapshot (= N_STEPS)
INTERACTIONS = 200        # = N_AGENTS
NICHE_WIDTH = 0.10        # moderate
CONFIDENCE_THRESHOLD = 0.5  # agent "accepts" if confidence > this
DECLINE_THRESHOLD = -0.01   # delta must be below this to count as declining
SELECTION_WINDOW = (0.20, 0.50)  # young acceptance rate range for selection


# ============================================================================
# CORE SIMULATION
# ============================================================================

def generate_feature_population(n_features: int, seed: int) -> List[float]:
    """Generate initial user fractions for a synthetic feature set."""
    rng = random.Random(seed)
    return [rng.uniform(0.01, 0.99) for _ in range(n_features)]


def run_single_feature(initial_users: float, rho: float,
                       seed: int) -> Tuple[float, float]:
    """
    Run ABM for one feature. Return (old_acceptance, young_acceptance).

    Acceptance = fraction of agents with confidence > CONFIDENCE_THRESHOLD.
    """
    cxn = Construction(
        name="feature",
        niche_width=NICHE_WIDTH,
        initial_users=initial_users,
        initial_confidence=(6, 2),
    )
    world = World(
        constructions=[cxn],
        n_agents=N_AGENTS,
        rho=rho,
        random_seed=seed,
    )

    # Run to old snapshot
    world.run(n_steps=OLD_SNAPSHOT, interactions=INTERACTIONS)
    old_confs = [a.beliefs["feature"].confidence for a in world.agents]
    old_rate = sum(1 for c in old_confs if c > CONFIDENCE_THRESHOLD) / N_AGENTS

    # Continue to young snapshot
    world.run(n_steps=YOUNG_SNAPSHOT - OLD_SNAPSHOT, interactions=INTERACTIONS)
    young_confs = [a.beliefs["feature"].confidence for a in world.agents]
    young_rate = sum(1 for c in young_confs if c > CONFIDENCE_THRESHOLD) / N_AGENTS

    return old_rate, young_rate


def estimate_rho(features_data: List[Tuple[float, float, float]]) -> Tuple[
        Optional[float], int, List[float]]:
    """
    Apply the SCOSYA estimation method to synthetic feature data.

    Args:
        features_data: list of (initial_users, old_rate, young_rate)

    Returns:
        (estimated_rho, n_qualifying, qualifying_young_rates)
    """
    qualifying = []
    qualifying_rates = []

    for init_f, old_rate, young_rate in features_data:
        delta = young_rate - old_rate

        # Filter: declining AND young rate in selection window
        if (delta < DECLINE_THRESHOLD
                and SELECTION_WINDOW[0] <= young_rate <= SELECTION_WINDOW[1]):
            rho_est = young_rate / (1 - young_rate)
            qualifying.append(rho_est)
            qualifying_rates.append(young_rate)

    if not qualifying:
        return None, 0, []

    return statistics.median(qualifying), len(qualifying), qualifying_rates


def run_one_replicate(true_rho: float, replicate: int,
                      base_seed: int) -> Dict:
    """Run a full replicate: 258 features, estimate rho."""
    seed_offset = base_seed + replicate * 10000

    # Generate feature population (same features across rho for fairness)
    initial_fracs = generate_feature_population(N_FEATURES, seed_offset)

    features_data = []
    for i, init_f in enumerate(initial_fracs):
        old_rate, young_rate = run_single_feature(
            init_f, true_rho, seed_offset + i + 1)
        features_data.append((init_f, old_rate, young_rate))

    rho_hat, n_qual, qual_rates = estimate_rho(features_data)

    # Also compute some diagnostics
    n_declining = sum(1 for _, o, y in features_data
                      if y - o < DECLINE_THRESHOLD)
    n_alive = sum(1 for _, _, y in features_data if y > 0.5)
    n_dead = sum(1 for _, _, y in features_data if y < 0.1)

    return {
        "true_rho": true_rho,
        "replicate": replicate,
        "rho_hat": rho_hat,
        "n_qualifying": n_qual,
        "n_declining": n_declining,
        "n_alive": n_alive,
        "n_dead": n_dead,
        "qualifying_rates": qual_rates,
        "features_data": features_data,
    }


# ============================================================================
# MAIN EXPERIMENT
# ============================================================================

def run_calibration(n_replicates: int = 20, no_viz: bool = False):
    """Run the full simulation-based calibration."""
    print("=" * 70)
    print("SIMULATION-BASED CALIBRATION OF RHO ESTIMATION METHOD")
    print("=" * 70)
    print(f"\n  True rho values: {TRUE_RHO_VALUES}")
    print(f"  Features per replicate: {N_FEATURES}")
    print(f"  Agents per feature: {N_AGENTS}")
    print(f"  Steps: {N_STEPS} (old snapshot at {OLD_SNAPSHOT})")
    print(f"  MC replicates per rho: {n_replicates}")
    print(f"  Selection window: {SELECTION_WINDOW}")
    print(f"  Decline threshold: {DECLINE_THRESHOLD}")
    print(f"  Total ABM runs: {len(TRUE_RHO_VALUES) * N_FEATURES * n_replicates}")
    print()

    all_results = {}  # rho -> list of replicate results

    total_t0 = time.time()

    for rho_idx, true_rho in enumerate(TRUE_RHO_VALUES):
        base_seed = (rho_idx + 1) * 100000
        rho_t0 = time.time()

        print(f"\n--- True rho = {true_rho} ---")
        print(f"  Analytic critical mass: {true_rho / (1 + true_rho):.3f}")

        replicates = []
        for rep in range(n_replicates):
            result = run_one_replicate(true_rho, rep, base_seed)
            replicates.append(result)

            if (rep + 1) % 5 == 0 or rep == 0:
                est = f"{result['rho_hat']:.3f}" if result['rho_hat'] is not None else "NaN"
                print(f"    rep {rep+1:>2}/{n_replicates}: "
                      f"rho_hat={est}  "
                      f"n_qual={result['n_qualifying']:>3}  "
                      f"n_declining={result['n_declining']:>3}  "
                      f"alive/dead={result['n_alive']}/{result['n_dead']}")

        all_results[true_rho] = replicates
        elapsed = time.time() - rho_t0
        print(f"  [{elapsed:.1f}s]")

    total_elapsed = time.time() - total_t0

    # ===================================================================
    # RESULTS
    # ===================================================================
    print("\n" + "=" * 70)
    print("RESULTS")
    print("=" * 70)

    print(f"\n{'True rho':>10} {'Median est':>12} {'Mean est':>10} "
          f"{'Bias':>8} {'IQR':>14} {'Med n_qual':>12} {'NaN reps':>10}")
    print(f"  {'-'*78}")

    summary = {}

    for true_rho in TRUE_RHO_VALUES:
        reps = all_results[true_rho]
        estimates = [r["rho_hat"] for r in reps if r["rho_hat"] is not None]
        n_nan = sum(1 for r in reps if r["rho_hat"] is None)
        n_quals = [r["n_qualifying"] for r in reps]

        if estimates:
            med_est = statistics.median(estimates)
            mean_est = statistics.mean(estimates)
            bias = med_est - true_rho
            q25 = sorted(estimates)[max(0, len(estimates) // 4)]
            q75 = sorted(estimates)[min(len(estimates) - 1,
                                        3 * len(estimates) // 4)]
            med_nqual = statistics.median(n_quals)

            summary[true_rho] = {
                "median": med_est, "mean": mean_est, "bias": bias,
                "q25": q25, "q75": q75, "n_nan": n_nan,
                "med_nqual": med_nqual, "estimates": estimates,
            }

            print(f"  {true_rho:>8.1f} {med_est:>12.3f} {mean_est:>10.3f} "
                  f"{bias:>+8.3f} [{q25:.3f}, {q75:.3f}] "
                  f"{med_nqual:>12.0f} {n_nan:>10}")
        else:
            summary[true_rho] = {
                "median": None, "mean": None, "bias": None,
                "q25": None, "q75": None, "n_nan": n_nan,
                "med_nqual": statistics.median(n_quals),
                "estimates": [],
            }
            print(f"  {true_rho:>8.1f} {'ALL NaN':>12} {'':>10} "
                  f"{'':>8} {'':>14} "
                  f"{statistics.median(n_quals):>12.0f} {n_nan:>10}")

    # ===================================================================
    # VERDICT
    # ===================================================================
    print("\n" + "=" * 70)
    print("VERDICT")
    print("=" * 70)

    # Check prediction 1: positive bias at low rho
    if summary[0.2]["bias"] is not None:
        p1 = summary[0.2]["bias"] > 0
        print(f"\n  Prediction 1 (positive bias at rho=0.2): "
              f"bias={summary[0.2]['bias']:+.3f} -> "
              f"{'CONFIRMED' if p1 else 'REJECTED'}")

    # Check prediction 2: best recovery at rho ~ 0.5
    biases = {r: abs(s["bias"]) for r, s in summary.items()
              if s["bias"] is not None}
    if biases:
        best_rho = min(biases, key=biases.get)
        print(f"  Prediction 2 (best recovery at rho~0.5): "
              f"lowest |bias| at rho={best_rho} -> "
              f"{'CONFIRMED' if best_rho in (0.5, 0.3) else 'REJECTED'}")

    # Check prediction 3: underestimation at rho = 1.0
    if summary[1.0]["bias"] is not None:
        p3 = summary[1.0]["bias"] < 0
        print(f"  Prediction 3 (underestimation at rho=1.0): "
              f"bias={summary[1.0]['bias']:+.3f} -> "
              f"{'CONFIRMED' if p3 else 'REJECTED'}")

    # Check prediction 5: few qualifying at extremes
    if summary[0.2]["med_nqual"] is not None and summary[0.5]["med_nqual"] is not None:
        p5 = (summary[0.2]["med_nqual"] < summary[0.5]["med_nqual"]
               or summary[1.0]["med_nqual"] < summary[0.5]["med_nqual"])
        print(f"  Prediction 5 (fewer qualifying at extremes): "
              f"n_qual at 0.2={summary[0.2]['med_nqual']:.0f}, "
              f"0.5={summary[0.5]['med_nqual']:.0f}, "
              f"1.0={summary[1.0]['med_nqual']:.0f} -> "
              f"{'CONFIRMED' if p5 else 'REJECTED'}")

    # The big question: can we distinguish rho values?
    print(f"\n  DISTINGUISHABILITY:")
    for i in range(len(TRUE_RHO_VALUES) - 1):
        r1, r2 = TRUE_RHO_VALUES[i], TRUE_RHO_VALUES[i + 1]
        s1, s2 = summary[r1], summary[r2]
        if s1["q25"] is not None and s2["q75"] is not None:
            overlap = s1["q75"] > s2["q25"]
            print(f"    rho={r1} vs rho={r2}: "
                  f"[{s1['q25']:.3f}, {s1['q75']:.3f}] vs "
                  f"[{s2['q25']:.3f}, {s2['q75']:.3f}] -> "
                  f"{'OVERLAP (cannot distinguish)' if overlap else 'SEPARATED (distinguishable)'}")

    # What this means for our SCOSYA estimate
    print(f"\n  IMPLICATIONS FOR SCOSYA ESTIMATE (rho ~ 0.5):")
    if summary[0.5]["bias"] is not None:
        correction = summary[0.5]["bias"]
        print(f"    Bias at true rho=0.5: {correction:+.3f}")
        print(f"    Corrected SCOSYA estimate: {0.5 - correction:.2f}")
        if summary[0.5]["q25"] is not None:
            spread = summary[0.5]["q75"] - summary[0.5]["q25"]
            print(f"    Estimation precision (IQR width): {spread:.3f}")

    print(f"\n  Total wall time: {total_elapsed:.1f}s")

    # ===================================================================
    # VISUALIZATION
    # ===================================================================
    if HAS_MPL and not no_viz:
        plot_calibration(summary, all_results)

    return summary, all_results


# ============================================================================
# VISUALIZATION
# ============================================================================

def plot_calibration(summary, all_results):
    """Three-panel calibration diagnostic plot."""
    fig, axes = plt.subplots(1, 3, figsize=(16, 5))

    # Panel 1: True vs estimated rho (the money plot)
    ax = axes[0]
    rhos = [r for r in TRUE_RHO_VALUES if summary[r]["median"] is not None]
    medians = [summary[r]["median"] for r in rhos]
    q25s = [summary[r]["q25"] for r in rhos]
    q75s = [summary[r]["q75"] for r in rhos]

    ax.plot([0, 1.2], [0, 1.2], "k--", alpha=0.3, label="Perfect recovery")
    ax.errorbar(rhos, medians,
                yerr=[[m - q for m, q in zip(medians, q25s)],
                      [q - m for m, q in zip(medians, q75s)]],
                fmt="o-", color="#2c3e50", linewidth=2, markersize=8,
                capsize=5, label="Median estimate (IQR)")

    # Show individual estimates as jittered points
    for rho in rhos:
        ests = summary[rho]["estimates"]
        jitter = [rho + random.gauss(0, 0.015) for _ in ests]
        ax.scatter(jitter, ests, alpha=0.2, s=15, color="#3498db", zorder=1)

    ax.set_xlabel("True rho", fontsize=11)
    ax.set_ylabel("Estimated rho", fontsize=11)
    ax.set_title("Recovery: True vs Estimated rho", fontsize=12)
    ax.legend(fontsize=9)
    ax.set_xlim(0, 1.15)
    ax.set_ylim(0, 1.5)
    ax.grid(True, alpha=0.2)

    # Panel 2: Number of qualifying features
    ax = axes[1]
    for rho in TRUE_RHO_VALUES:
        n_quals = [r["n_qualifying"] for r in all_results[rho]]
        jitter_x = [rho + random.gauss(0, 0.015) for _ in n_quals]
        ax.scatter(jitter_x, n_quals, alpha=0.4, s=20, color="#e67e22")
    med_nquals = [summary[r]["med_nqual"] for r in TRUE_RHO_VALUES]
    ax.plot(TRUE_RHO_VALUES, med_nquals, "o-", color="#c0392b",
            linewidth=2, markersize=8, label="Median")
    ax.set_xlabel("True rho", fontsize=11)
    ax.set_ylabel("# qualifying features", fontsize=11)
    ax.set_title("Qualifying Features per Replicate", fontsize=12)
    ax.legend(fontsize=9)
    ax.grid(True, alpha=0.2)

    # Panel 3: Bias
    ax = axes[2]
    biases = [summary[r]["bias"] if summary[r]["bias"] is not None else 0
              for r in TRUE_RHO_VALUES]
    colors = ["#e74c3c" if b > 0 else "#2ecc71" for b in biases]
    ax.bar(range(len(TRUE_RHO_VALUES)), biases, color=colors,
           edgecolor="white", alpha=0.8)
    ax.set_xticks(range(len(TRUE_RHO_VALUES)))
    ax.set_xticklabels([str(r) for r in TRUE_RHO_VALUES])
    ax.set_xlabel("True rho", fontsize=11)
    ax.set_ylabel("Bias (median estimate - true)", fontsize=11)
    ax.set_title("Estimation Bias", fontsize=12)
    ax.axhline(0, color="black", linewidth=0.5)
    ax.grid(True, alpha=0.2, axis="y")

    fig.suptitle("Simulation-Based Calibration: Can We Recover rho?",
                 fontsize=14, fontweight="bold")
    plt.tight_layout()
    plt.savefig("fake_data_calibration.png", dpi=150)
    print(f"\nPlot saved: fake_data_calibration.png")
    plt.close()


# ============================================================================
# MAIN
# ============================================================================

def main():
    parser = argparse.ArgumentParser(
        description="Simulation-based calibration of rho estimation",
        formatter_class=argparse.RawDescriptionHelpFormatter)
    parser.add_argument("--mc-runs", type=int, default=20,
                        help="Replicates per rho value (default: 20)")
    parser.add_argument("--no-viz", action="store_true")
    args = parser.parse_args()

    run_calibration(n_replicates=args.mc_runs, no_viz=args.no_viz)


if __name__ == "__main__":
    main()
